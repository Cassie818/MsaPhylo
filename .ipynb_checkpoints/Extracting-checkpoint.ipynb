{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159965dd8885eca0",
   "metadata": {},
   "source": [
    "# Deciphering the Black Box: Mastering the MSA Transformer for Phylogenetic Tree Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b7b4fa8508565",
   "metadata": {},
   "source": [
    "## Install and import package ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1abb88b67515501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T01:21:33.690095Z",
     "start_time": "2023-09-23T01:21:33.676039Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from Bio import SeqIO\n",
    "from ete3 import Tree\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, MSATransformer, pretrained\n",
    "from pysam import FastaFile, FastxFile\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63678f3b533734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = './Embeddings/Pfam/'\n",
    "ATTN_PATH = './Attentions/Pfam/'\n",
    "MSA_PATH = './data/Pfam/'\n",
    "\n",
    "MSA_TYPE_MAP = {\n",
    "    \"default\": \".fasta\",\n",
    "    \"sc\": \"_shuffle_column.fasta\",\n",
    "    \"sa\": \"_shuffle_all.fasta\",\n",
    "    \"mc\": \"_mix_column.fasta\"\n",
    "}\n",
    "\n",
    "EMB_TYPE_MAP = {\n",
    "    \"default\": \"_emb_\",\n",
    "    \"sc\": \"_emb_shuffle_column_\",\n",
    "    \"sa\": \"_emb_shuffle_all_\",\n",
    "    \"mc\": \"_emb_mix_column_\"\n",
    "}\n",
    "\n",
    "ATTN_TYPE_MAP = {\n",
    "    \"default\": \"_attn_\",\n",
    "    \"sc\": \"_attn_shuffle_column_\",\n",
    "    \"sa\": \"_attn_shuffle_all_\",\n",
    "    \"mc\": \"_attn_mix_column_\"\n",
    "}\n",
    "\n",
    "\n",
    "def remove_insertions(sequence):\n",
    "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "    deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "    deletekeys[\".\"] = None\n",
    "    deletekeys[\"*\"] = None\n",
    "    translation = str.maketrans(deletekeys)\n",
    "    return sequence.translate(translation)\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "\n",
    "    def __init__(self, protein_family, msa_type):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = \"esm_msa1b_t12_100M_UR50S\"\n",
    "        self.encoding_dim, self.encoding_layer, self.max_seq_length = 768, 12, 1022\n",
    "        self.protein_family = protein_family\n",
    "        self.msa_type = msa_type\n",
    "        self.msa_fasta_file = f'{MSA_PATH}{protein_family}{MSA_TYPE_MAP[self.msa_type]}'\n",
    "\n",
    "    def read_msa(self):\n",
    "        return [(record.description, remove_insertions(str(record.seq)))\n",
    "                for record in SeqIO.parse(self.msa_fasta_file, \"fasta\")]\n",
    "\n",
    "    def get_embedding(self):\n",
    "        model, alphabet = pretrained.load_model_and_alphabet(self.model_name)\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "        emb = f'{EMB_PATH}{self.protein_family}{EMB_TYPE_MAP[self.msa_type]}{self.model_name}.pt'\n",
    "        plm_embedding = {}\n",
    "\n",
    "        model.eval()\n",
    "        msa_data = [self.read_msa()]\n",
    "        msa_labels, msa_strs, msa_tokens = batch_converter(msa_data)\n",
    "        seq_num = len(msa_labels[0])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for layer in range(self.encoding_layer):\n",
    "                out = model(msa_tokens, repr_layers=[layer], return_contacts=False)\n",
    "                token_representations = out[\"representations\"][layer].view(seq_num, -1,\n",
    "                                                                           self.encoding_dim)\n",
    "                # remove the start token\n",
    "                token_representations = token_representations[:, 1:, :]\n",
    "                print(f\"Finish extracting embeddings from layer {layer}.\")\n",
    "                plm_embedding[layer] = token_representations\n",
    "\n",
    "        torch.save(plm_embedding, emb)\n",
    "        print(\"Embeddings saved in output file:\", emb)\n",
    "\n",
    "    def get_col_attention(self):\n",
    "        model, alphabet = pretrained.load_model_and_alphabet(self.model_name)\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "        attn = f'{ATTN_PATH}{self.protein_family}{ATTN_TYPE_MAP[self.msa_type]}{self.model_name}.pt'\n",
    "\n",
    "        model.eval()\n",
    "        msa_data = [self.read_msa()]\n",
    "        msa_labels, msa_strs, msa_tokens = batch_converter(msa_data)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = model(msa_tokens, repr_layers=[12], need_head_weights=True)\n",
    "\n",
    "        torch.save(results, attn)\n",
    "        print(\"Column attention saved in output file:\", attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3bfa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_family_list = ['PF00080_gap']\n",
    "msa_type_list = ['default']\n",
    "for protein_family in protein_family_list:\n",
    "    for msa_type in msa_type_list:\n",
    "        ext = Extractor(protein_family, msa_type)\n",
    "        # ext.get_embedding()\n",
    "        ext.get_col_attention()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

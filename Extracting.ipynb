{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deciphering the Black Box: Mastering the MSA Transformer for Phylogenetic Tree Construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "159965dd8885eca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.Install and import package ##"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff6b7b4fa8508565"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-22T05:23:40.022087Z",
     "start_time": "2023-09-22T05:23:29.847029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Bio in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: mygene in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from Bio) (3.2.2)\r\n",
      "Requirement already satisfied: biopython>=1.79 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from Bio) (1.79)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from Bio) (2.28.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from Bio) (4.64.1)\r\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from biopython>=1.79->Bio) (1.21.5)\r\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from mygene->Bio) (0.2.6)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from requests->Bio) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from requests->Bio) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from requests->Bio) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from requests->Bio) (3.3)\r\n",
      "Requirement already satisfied: ete3 in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (3.1.3)\r\n",
      "Requirement already satisfied: dendropy in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (4.6.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/tf37/lib/python3.7/site-packages (from dendropy) (61.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install pysam --quiet\n",
    "!pip install Bio\n",
    "!pip install ete3\n",
    "!pip install dendropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import esm\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "from pysam import FastaFile,FastxFile\n",
    "from torch.utils.data import TensorDataset,Dataset\n",
    "import pandas as pd\n",
    "from Bio import SeqIO"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T05:24:30.801720Z",
     "start_time": "2023-09-22T05:24:30.797731Z"
    }
   },
   "id": "f1abb88b67515501"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Extracting:\n",
    "\n",
    "  def __init__(self, msa_fasta_file, protein_family, file_location):\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.model_name = \"esm_msa1b_t12_100M_UR50S\"\n",
    "    self.protein_family = protein_family\n",
    "    self.msa_fasta_file = msa_fasta_file\n",
    "    self.file_location = file_location\n",
    "    self.encoding_dim, self.encoding_layer, self.max_seq_length = 768, 12, 1022\n",
    "\n",
    "  def process_fasta_file(self):\n",
    "    msa = []\n",
    "    labels = [record.id for record in SeqIO.parse(self.msa_fasta_file, \"fasta\")]\n",
    "    for record in SeqIO.parse(self.msa_fasta_file, \"fasta\"):\n",
    "      sequence = str(record.seq)\n",
    "      self.sequence_length = len(sequence)\n",
    "      if self.sequence_length > self.max_seq_length: # if the sequence length over the maximum value\n",
    "        sequence = sequence[:self.max_seq_length]\n",
    "        self.sequence_length = self.max_seq_length\n",
    "      sequence_id = record.id\n",
    "      msa.append((sequence_id, sequence))\n",
    "    return msa,labels\n",
    "\n",
    "\n",
    "  # process fasta file into pytorch dataset and dataloader\n",
    "  def get_plm_msa_embedding(self):\n",
    "    model, alphabet = pretrained.load_model_and_alphabet(self.model_name)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    tokens_to_index = alphabet.tok_to_idx.copy()\n",
    "    output_file_name = self.file_location + self.protein_family + '_all_layers_' + self.model_name + '.pt'\n",
    "    print(\"Total Tokens\",len(tokens_to_index))\n",
    "    print(\"Model Name\",model)\n",
    "    print(\"Encoding Dim size\",self.encoding_dim)\n",
    "    print(\"Representation Layer\",self.encoding_layer)\n",
    "\n",
    "    model.eval()\n",
    "    # process fasta file into pytorch dataset\n",
    "    MSA,labels = self.process_fasta_file()\n",
    "    # convert all batches into model embeddings\n",
    "    plm_embedding = {}\n",
    "    msa_labels, msa_strs, msa_tokens = batch_converter(MSA)\n",
    "    with torch.no_grad():\n",
    "      for layer in range(self.encoding_layer):\n",
    "        out = model(msa_tokens, repr_layers=[layer], return_contacts = False)\n",
    "        token_representations = out[\"representations\"][layer].view(-1, self.sequence_length+1, self.encoding_dim)\n",
    "        token_representations = token_representations[:,1:,:]\n",
    "        print(f\"Finish extracting embeddings from layer {layer}.\")\n",
    "        plm_embedding[layer] = token_representations\n",
    "\n",
    "    torch.save(plm_embedding, output_file_name)\n",
    "    print(\"Embeddings saved in output file:\", output_file_name)\n",
    "\n",
    "  def get_col_attention(self):\n",
    "    col_file_name = self.file_location + self.protein_family + '_attention_' + self.model_name + '.pt'\n",
    "    col_attention_file = self.file_location + '/' + self.protein_family + '/col_attention.csv'\n",
    "    model, alphabet = pretrained.load_model_and_alphabet(self.model_name)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    tokens_to_index = alphabet.tok_to_idx.copy()\n",
    "    model.eval()\n",
    "    # process fasta file into pytorch dataset\n",
    "    MSA,labels = self.process_fasta_file()\n",
    "    # convert all batches into model embeddings\n",
    "    plm_embedding = {}\n",
    "    msa_labels, msa_strs, msa_tokens = batch_converter(MSA)\n",
    "    with torch.no_grad():\n",
    "      for layer in range(self.encoding_layer):\n",
    "        out = model(msa_tokens, repr_layers=[12],need_head_weights=True)\n",
    "    torch.save(out, col_file_name)\n",
    "    print(\"Embeddings saved in output file:\", col_file_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f032996cc3252c6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
